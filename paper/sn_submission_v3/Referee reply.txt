(1) The work as presented is in almost acceptable form. The part about using just the
first few points is powerful and will be welcomed by many. If using the entire
light-curve during testing provides somewhat worse results, then perhaps there
is a way to improve the model even further.

There is possibly a way to improve the model further, but this is not certain. One of our quarms about trying to improve the model further is that the way RNNs work mean that including the information from after the early epoch challenge skews the overall weights of the network meaning that the early epoch weights are less well known compared to training using only the early epoch challenge data.

(2) Many machine learning methods seem like black-boxes and keeping them that way
does not help dispel myths about them from the eyes of the uninitiated. When
describing example data, can some stats be provided about the amount of
chunking in light-curves? For example the range of time-span for the
light-curves, the number of chunks they are typically divided in (a box-plot
could be used to describe median, show outliers etc.) Or point to some
specifics from the SPCC figures (table X, Fig Y etc.)

(3) *ANY* discussion about error-bars will be helpful in letting readers decide if
they would want to apply the technique. [See comment below about 73+/-2%]. But
thats just a suggestion and not a requirement.

The errors in the results are due to running the training 5 times with different random selections of training data (and random augmentations of data). This is mentioned in the results section already.

(4) Did the authors try removing known points, then apply their chunking and random
filling and see if they get consistent results?

To do...

(5) The discussion about padding is unclear. if, in the end, the padding does not
get used, may be the discussion should be removed to an appendix or a footnote
(but do not remove it as it is part of the flow). Perhaps an example will help?

The padding is used to create the vectors of the correct length. This padding is then ignored by the network using a technique called masking. This means the padding has no effect on the results, it just maintains the correct length vectors. This is already explicitly described in the text.

(6a) In Table 2 please include the 0.052% even if the results are bad. 

They are already in the table in every case - this a train fraction of 0.052, not 0.052%.

(6b) Are the early-epoch results for SN1a or 123? Can the results be provided for both sets?

The early-epoch challenge is for SN1a only and not 123. We have given that a go and added the results to the table.

(6c) In the middle section, in the last 2 lines, Completeness seems to rise when Host
information is dropped. Thats anomalous. If you understand why, a comment will
be useful. Otherwise those are the kind of error-bars that become meaningful:
are both 73+/-2%? You can still fit the table on a single page by abbreviating
contents in the first column.

This was a statistical fluctuation. We have added the errors on the purity and completeness. As it can be seen the errors on both of the bottom lines of the second section are ~4%, approximately the same as the difference between the means.

(7) It is understandable that your methods are not comparable with various methods
used during SPCC, but perhaps its still worth mentioning the best numbers
there?

To do...

(8) The early-epoch result is surprising. May be it should be emphasized more? It
will be useful to many people.

To do..

(9) The following is a loose sentence: "Moreover, we have ...trained on the whole
light-curve". Especially so since you do not normalize time to the peak value so the
starting point could be arbitrary (or is your padding mitigating exactly that?
if so, clarify a little). Otherwise quantify (at least verbally) what you mean
by the above sentence.

We mean that training using the data from the early epoch challenge gives a better early-time predictor than training using the whole light-curve. We have rewritten the sentence to make it clearer.

(10) You mention that perhaps even raw images could be used etc., but you have not
even discussed the effects of different filters etc. What happens if you use
just one or two filters, for instance? So, either go into more details, or
exclude pure speculation.

This isn't pure speculation, this is a consideration for further work. It is possible to data mine images and pipe this into an RNN. We could do this for a single filter, we could do this for many filters. If one filter was missing then the input could be augmented as done here and the other raw images convolved to extract features. This is identical to what is done here, but with a huge amount more information.

(11) Section 4: Over-fitting can also show up as some inputs being learnt too well
(and thus its not the noise that gets fitted)

What is different between differences in inputs and noise? They are the same as far as we can tell

(12) You mention in the comments that the code is public. Please refer to it so that
it can be used by others.

This is already mentioned, the link is on the front page.

(13) Shorten the y-axes in figure 3 (keeping it same for both parts) to reduce
unused white-space.

Done.

minor comments:
(1) references need to be delineated better - many seem to be inline now and don't feel right.

We are not sure how it's supposed to feel?

(2) inputted -> input

We have changed this